# main.py
import os
import tempfile
from typing import Any, Dict, List

import streamlit as st
from files import file_uploader, url_uploader
from question import chat_with_doc
from brain import brain
from langchain.docstore.document import Document
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import SupabaseVectorStore
from supabase import Client, create_client
from explorer import view_document


supabase_url = st.secrets.supabase_url
supabase_key = st.secrets.supabase_service_key
openai_api_key = st.secrets.openai_api_key
anthropic_api_key = st.secrets.anthropic_api_key
supabase: Client = create_client(supabase_url, supabase_key)
self_hosted = st.secrets.self_hosted


class CustomSupabaseVectorStore(SupabaseVectorStore):
    '''A custom vector store that uses the match_vectors table instead of the vectors table.'''
    def __init__(self, client: Client, embedding: OpenAIEmbeddings, table_name: str):
        super().__init__(client, embedding, table_name)
    
    def similarity_search(
        self, 
        query: str, 
        table: str = "match_vectors", 
        k: int = 3, 
        threshold: float = 0.5, 
        **kwargs: Any
    ) -> List[Document]:
        vectors = self._embedding.embed_documents([query])
        query_embedding = vectors[0]
        res = self._client.rpc(
            table,
            {
                "query_embedding": query_embedding,
                "match_count": k,
            },
        ).execute()

        match_result = [
            (
                Document(
                    metadata=search.get("metadata", {}),  # type: ignore
                    page_content=search.get("content", ""),
                ),
                search.get("similarity", 0.0),
            )
            for search in res.data
            if search.get("content")
        ]

        documents = [doc for doc, _ in match_result]

        return documents
    

embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
vector_store = CustomSupabaseVectorStore(
    supabase, embeddings, table_name="vectors")
models = ["gpt-3.5-turbo", "gpt-4"]

# Set the theme
st.set_page_config(
    page_title="Docu-Chat",
    layout="wide",
    initial_sidebar_state="expanded",
)


st.title("ğŸ“„ğŸ“¢ Docu-Chat (Proto)")
st.markdown("è³‡æ–™ã‚’è¿½åŠ ã™ã‚‹ã¨ã€ãã®è³‡æ–™ã®å†…å®¹ã«ã‚‚ã¨ã¥ã„ã¦ç­”ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚")

# Initialize session state variables
if 'model' not in st.session_state:
    st.session_state['model'] = "gpt-3.5-turbo"
if 'temperature' not in st.session_state:
    st.session_state['temperature'] = 0.1
if 'chunk_size' not in st.session_state:
    st.session_state['chunk_size'] = 500
if 'chunk_overlap' not in st.session_state:
    st.session_state['chunk_overlap'] = 0
if 'max_tokens' not in st.session_state:
    st.session_state['max_tokens'] = 1024


# Create a radio button for user to choose between adding knowledge or asking a question
user_choice = st.sidebar.radio(
    "ä½•ã‚’ã—ã¾ã™ã‹ï¼Ÿ", ('ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ', 'ãƒãƒ£ãƒƒãƒˆã™ã‚‹', 'ãƒ‡ãƒ¼ã‚¿å‰Šé™¤', "ãƒ‡ãƒ¼ã‚¿ç¢ºèª", "èª¬æ˜"), index=4)


if user_choice == 'ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ':
    # Display chunk size and overlap selection only when adding knowledge
    st.sidebar.title("è¨­å®š")
    st.sidebar.markdown(
        "Choose your chunk size and overlap for adding knowledge.")
    st.session_state['chunk_size'] = st.sidebar.slider(
        "Select Chunk Size", 100, 1000, st.session_state['chunk_size'], 50)
    st.session_state['chunk_overlap'] = st.sidebar.slider(
        "Select Chunk Overlap", 0, 100, st.session_state['chunk_overlap'], 10)
    
    # Create two columns for the file uploader and URL uploader
    col1, col2 = st.columns(2)
    
    with col1:
        file_uploader(supabase, vector_store)
    with col2:
        url_uploader(supabase, vector_store)
elif user_choice == 'ãƒãƒ£ãƒƒãƒˆã™ã‚‹':
    # Display model and temperature selection only when asking questions
    st.sidebar.title("è¨­å®š")
    st.sidebar.markdown(
        "ãƒ¢ãƒ‡ãƒ«ã€æ¸©åº¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

    st.session_state['model'] = st.sidebar.selectbox(
    "Select Model", models, index=(models).index(st.session_state['model']))

    st.session_state['temperature'] = st.sidebar.slider(
        "Select Temperature", 0.0, 1.0, st.session_state['temperature'], 0.1)

    st.session_state['max_tokens'] = st.sidebar.slider(
        "Select Max Tokens", 256, 4096, st.session_state['max_tokens'], 1)
    
    chat_with_doc(st.session_state['model'], vector_store, stats_db=supabase)
elif user_choice == 'ãƒ‡ãƒ¼ã‚¿å‰Šé™¤':
    st.sidebar.title("è¨­å®š")
    brain(supabase)
elif user_choice == 'ãƒ‡ãƒ¼ã‚¿ç¢ºèª':
    st.sidebar.title("è¨­å®š")
    view_document(supabase)
elif user_choice == "èª¬æ˜":
    st.sidebar.title("è¨­å®š")
    st.write("## ã“ã®ã‚¢ãƒ—ãƒªã§ã§ãã‚‹ã“ã¨")
    st.markdown("""
                ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã¨ã€è¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ç¾¤ã®ä¸­ã‹ã‚‰è¿‘ã—ã„ã¨æ€ã‚ã‚Œã‚‹ã‚‚ã®ã‚’å‚ç…§ã—ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚\n
                **ãƒãƒ£ãƒƒãƒˆã®ä¸‹ã«ã¯ãã®å›ç­”ã®åŸºã¨ãªã£ãŸãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚‚è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚**\n
                ä¸€èˆ¬çš„ãªè³ªå•ãªã©ã«ã¯å›ç­”ã§ããªããªã£ã¦ã„ã¾ã™ã®ã§ã€ãã†ã„ã£ãŸã‚‚ã®ã¯\n
                - Googleã®Bard
                - Microsoftã®Bing
                - OpenAIã®ChatGPT\n
                ãªã©ã«èã„ã¦ãã ã•ã„ã€‚
                """)
    st.write("ä¸­èº«ã¯[ã“ã‚Œ](https://prtimes.jp/main/html/rd/p/000000078.000092586.html)ã«è¿‘ã„ã‹ã‚‚ã—ã‚Œãªã„ã§ã™ã€‚ã¨ã¦ã‚‚ã€‚")
    st.write("## ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ã«ã¤ã„ã¦")
    st.warning("""
                ç¾åœ¨ã ã‚Œã§ã‚‚ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹çŠ¶æ…‹ã§ã™ã®ã§ã€**æ©Ÿå¯†æƒ…å ±ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯æ§ãˆã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¢ç”¨ã«è‡ªä½œã—ãŸãƒ‡ãƒ¼ã‚¿ã‚„ãƒãƒƒãƒˆä¸Šã®æœ€æ–°è«–æ–‡ãªã©ã§åŠ¹æœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚**
                """, icon="âš ï¸")
    st.markdown("""
                è¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ã„ãã¤ã‹ã®å¡Šï¼ˆãƒãƒ£ãƒ³ã‚¯ï¼‰ã«åˆ†å‰²ã•ã‚Œã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ ¼ç´ã•ã‚Œã¾ã™ã€‚\n
                ãŸã¨ãˆã°å‰å¾ŒåŠã®2åˆ†å‰²ã«ãªã£ãŸå ´åˆã€è³ªå•ã®å†…å®¹ã‹ã‚‰å¾ŒåŠã®ãƒ‡ãƒ¼ã‚¿ãŒå‚ç…§ã•ã‚ŒãŸãŒã€è‚å¿ƒã®å›ç­”ã¯å‰åŠã«ã‚ã£ãŸå ´åˆã€è³ªå•ã«å¯¾ã—ã¦å¾—ã‚‰ã‚ŒãŸç­”ãˆãŒæ­£ç¢ºã§ãªã„ã¨ã„ã†ã“ã¨ãŒèµ·ã“ã‚Šãˆã¾ã™ã€‚\n
                ãƒ‡ãƒ¼ã‚¿è¿½åŠ å¾Œã«ä½•åˆ†å‰²ã•ã‚ŒãŸã‹ã‚’ç¢ºèªã—ã€:red[ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’èª¿æ•´]ã—ã¦å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚Šã€**:red[ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’æ„è­˜ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹]**ãªã©ã®é‹ç”¨ãŒå¿…è¦ã«ãªã‚Šãã†ã§ã™ã€‚
                """)
    st.write("## æ§‹é€ ")
    st.markdown("""
                ã“ã®ã‚¢ãƒ—ãƒªã¯ä»¥ä¸‹ã®4ã¤ã®ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ãƒ„ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
                1. OpenAIã®API
                2. Supbase
                3. LangChain
                4. Streamlit Sharing
                """)
    st.write("### OpenAIã®API")
    st.markdown("""
                ChatGPTã§ä¸€ä¸–ã‚’é¢¨é¡ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒãƒ£ãƒƒãƒˆã®éƒ¨åˆ†ã«ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n
                æœ‰æ–™ã§ã™ãŒé«˜æ©Ÿèƒ½ã‹ã¤APIã®ãŸã‚æ‰‹è»½ã§ä½¿ã„ã‚„ã™ã„ã§ã™ã€‚
                - gpt-3.5-turbo: $0.002 /1K tokens
                - gpt-4: $0.03 /1K tokens
                """)
    st.write("### Supabase")
    st.markdown("""
                ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n
                Postgresï¼ˆãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ç°¡å˜ã«æ§‹ç¯‰ã§ãã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãªã‚µãƒ¼ãƒ“ã‚¹ã€‚ç¾åœ¨ã¯ç„¡æ–™ç‰ˆã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€ãƒ‡ãƒ¼ã‚¿å®¹é‡ãŒå¢—ãˆã‚‹ã¨Proãƒ—ãƒ©ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n
                - Free Plan: $0
                    - 500MBã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
                    - 1GBã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
                    - 50MBã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãªã©ã€‚
                - Pro Plan: $25
                    - 8GBã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
                    - 100GBã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
                    - 5GBã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãªã©ã€‚\n
                Firebaseã®alternativeã‚‰ã—ã„ã§ã™ãŒã€Firebaseã‚’è©³ã—ãã¯çŸ¥ã‚Šã¾ã›ã‚“ã€‚
                """)
    st.write("### LangChain")
    st.markdown("""
                GPT-3ã®ã‚ˆã†ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLarge Language Model; LLMï¼‰ã®æ©Ÿèƒ½ã‚’æ‹¡å¼µã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚\n
                æ¤œç´¢ã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãªã©ã¨é€£æºã—ãŸã†ãˆã§LLMã«å›ç­”ã•ã›ã‚‹ã€ã¨ã„ã£ãŸã“ã¨ã«åˆ©ç”¨ã§ãã¾ã™ã€‚ä¾¿åˆ©ã€‚ç„¡æ–™ã€‚
                """)
    st.write("### Streamlit Sharing")
    st.markdown("""
                Streamlitã¯ã€æ©Ÿæ¢°å­¦ç¿’ç³»ãƒ‡ãƒ¼ã‚¿ã‚„ã‚°ãƒ©ãƒ•ã®WEBã‚¢ãƒ—ãƒªåŒ–ã‚’ç°¡å˜ã«ã™ã‚‹Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚\n
                Streamlit Sharingã¯ã€ãã‚“ãªStreamlitã§ä½œã‚‰ã‚ŒãŸã‚¢ãƒ—ãƒªã‚’ç°¡å˜ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€‚ãŸã ã—ã€ã‚ªãƒ¼ãƒ—ãƒ³ã€‚
                """)
    st.write("## ç¤¾å†…å®Ÿè£…ã¸ã®èª²é¡Œ")
    st.markdown("""
                1. æ˜ç¢ºãªæ´»ç”¨æ–¹æ³•ã®ææ¡ˆ
                2. é‹ç”¨ã‚³ã‚¹ãƒˆ
                3. å®Ÿè£…æ–¹æ³•
                4. æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨æ€§
                """)
    st.write("## ãƒ™ãƒ¼ã‚¹")
    st.markdown("""
                ãƒ™ãƒ¼ã‚¹ã¯Quivrã¨ã„ã†ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‚\n
                Docker-Composeã§ç«‹ã¡ä¸Šã’ã‚‹ã‚¹ã‚¿ã‚¤ãƒ«ã®ã¾ã¾WEBã‚¢ãƒ—ãƒªåŒ–ã«æŒ«æŠ˜ã—ã€Streamlitå½¢å¼ã«æ”¹ç·¨ã—ã¦ã„ã¾ã™ã€‚
                """)
    st.write("[Quivr](https://github.com/stangirard/quivr)")
st.markdown("---\n\n")